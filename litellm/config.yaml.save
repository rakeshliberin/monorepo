model_list:
  - model_name: gemini-2.5-flash ### RECEIVED MODEL NAME ###
    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
      model: gemini/gemini-2.5-flash ### MODEL NAME sent to `litellm.completion()` ###
#      api_base: https://generativelanguage.googleapis.com/v1beta/openai
      api_key: AIzaSyA1LoiqqFTfZBTOTRujKcaWdUeOKZQ-lX4
#"os.environ/AZURE_API_KEY_EU" # does os.getenv("AZURE_API_KEY_EU")
 #     rpm: 6      # [OPTIONAL] Rate limit for this deployment: in requests per minute (rpm)

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env

general_settings:
  master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env
